import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Dict, Tuple


class PositionalEncoding(nn.Module):
    """
    Positional encoding for the transformer model.
    """
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Tensor, shape [seq_len, batch_size, embedding_dim]
        """
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)


class HMMTransformer(nn.Module):
    """
    Transformer model for predicting the next token in a sequence
    generated by an HMM. Optionally can also predict belief states.
    """
    def __init__(
        self,
        vocab_size: int,
        d_model: int = 128,
        nhead: int = 4,
        num_layers: int = 3,
        dim_feedforward: int = 512,
        dropout: float = 0.1,
        predict_beliefs: bool = False,
        num_states: Optional[int] = None
    ):
        """
        Args:
            vocab_size: Size of the vocabulary
            d_model: Dimension of the model
            nhead: Number of heads in multi-headed attention
            num_layers: Number of transformer layers
            dim_feedforward: Dimension of feedforward network
            dropout: Dropout probability
            predict_beliefs: Whether to predict belief states
            num_states: Number of hidden states in the HMM (required if predict_beliefs=True)
        """
        super(HMMTransformer, self).__init__()

        self.predict_beliefs = predict_beliefs
        if predict_beliefs and num_states is None:
            raise ValueError("num_states must be provided if predict_beliefs=True")

        self.num_states = num_states
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.positional_encoding = PositionalEncoding(d_model, dropout)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True  # Use batch_first=True for convenience
        )

        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        # Output heads
        self.token_predictor = nn.Linear(d_model, vocab_size)

        if predict_beliefs:
            # For belief states, we need to ensure they sum to 1 (simplex constraint)
            self.belief_predictor = nn.Sequential(
                nn.Linear(d_model, num_states),
                # We'll apply softmax during the forward pass
            )

    def forward(
        self,
        src: torch.Tensor,
        src_mask: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            src: Source sequence [batch_size, seq_len]
            src_mask: Optional mask for the source sequence

        Returns:
            Dictionary containing token_logits and optionally belief_probs
        """
        # Create embeddings [batch_size, seq_len, d_model]
        embeddings = self.token_embedding(src)

        # Apply positional encoding (need to transpose for positional encoding)
        # [batch_size, seq_len, d_model] -> [seq_len, batch_size, d_model]
        embeddings = embeddings.transpose(0, 1)
        embeddings = self.positional_encoding(embeddings)
        # Back to [batch_size, seq_len, d_model]
        embeddings = embeddings.transpose(0, 1)

        # Apply transformer encoder
        encoder_output = self.transformer_encoder(embeddings, src_key_padding_mask=src_mask)

        # Predict next tokens
        token_logits = self.token_predictor(encoder_output)

        output = {'token_logits': token_logits}

        # Predict belief states if required
        if self.predict_beliefs:
            belief_logits = self.belief_predictor(encoder_output)
            belief_probs = F.softmax(belief_logits, dim=-1)
            output['belief_probs'] = belief_probs

        return output

    def generate(
        self,
        prompt: torch.Tensor,
        max_length: int,
        temperature: float = 1.0
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Generate a sequence starting from a prompt.

        Args:
            prompt: Starting sequence [batch_size, prompt_len]
            max_length: Maximum length to generate
            temperature: Sampling temperature

        Returns:
            Tuple of (generated_tokens, belief_probs) where belief_probs is None if predict_beliefs=False
        """
        device = prompt.device
        batch_size = prompt.size(0)
        generated = prompt.clone()

        beliefs = None
        if self.predict_beliefs:
            beliefs = []

        # Generate up to max_length
        for _ in range(max_length - prompt.size(1)):
            # Get model predictions
            with torch.no_grad():
                output = self.forward(generated)

            # Get the last token prediction for each sequence in the batch
            next_token_logits = output['token_logits'][:, -1, :] / temperature

            # Sample from the logits
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)

            # Concatenate to the growing sequence
            generated = torch.cat([generated, next_token], dim=1)

            # Store beliefs if needed
            if self.predict_beliefs:
                beliefs.append(output['belief_probs'][:, -1, :])

        # Convert beliefs list to tensor if needed
        if self.predict_beliefs:
            beliefs = torch.stack(beliefs, dim=1)

        return generated, beliefs


class BeliefLoss(nn.Module):
    """
    Compute loss between predicted and true belief states.
    Uses KL divergence to measure difference between probability distributions.
    """
    def __init__(self, reduction: str = 'mean'):
        super(BeliefLoss, self).__init__()
        self.reduction = reduction

    def forward(self, pred_beliefs: torch.Tensor, true_beliefs: torch.Tensor) -> torch.Tensor:
        """
        Args:
            pred_beliefs: Predicted belief states [batch_size, seq_len, num_states]
            true_beliefs: True belief states [batch_size, seq_len, num_states]

        Returns:
            KL divergence loss
        """
        # KL divergence: KL(true || pred)
        kl_div = F.kl_div(
            pred_beliefs.log(),  # Log of predicted probs
            true_beliefs,        # True probs
            reduction='none'
        )

        # Sum over the state dimension
        kl_div = kl_div.sum(-1)

        if self.reduction == 'mean':
            return kl_div.mean()
        elif self.reduction == 'sum':
            return kl_div.sum()
        else:
            return kl_div
