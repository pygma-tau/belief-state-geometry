import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional, Union
from hmm import HMM, create_z1r_hmm, create_mess3_hmm, create_rrxor_hmm

class HMMDataset(Dataset):
    """PyTorch Dataset for sequences generated by Hidden Markov Models"""

    def __init__(
        self,
        hmm: Union[HMM, str],
        num_sequences: int = 1000,
        seq_length: int = 100,
        include_beliefs: bool = False,
        include_states: bool = False,
        token_to_idx: Optional[Dict[str, int]] = None,
        seed: Optional[int] = None
    ):
        """
        Args:
            hmm: Either an HMM instance or a string identifying a predefined HMM
                 ('z1r', 'mess3', 'rrxor')
            num_sequences: Number of sequences to generate
            seq_length: Length of each sequence
            include_beliefs: Whether to include belief states in the dataset
            include_states: Whether to include true hidden states in the dataset
            token_to_idx: Mapping from tokens to indices (if None, will be created)
            seed: Random seed for reproducibility
        """
        if seed is not None:
            np.random.seed(seed)
            torch.manual_seed(seed)

        # Initialize HMM
        if isinstance(hmm, str):
            if hmm.lower() == 'z1r':
                self.hmm = create_z1r_hmm()
            elif hmm.lower() == 'mess3':
                self.hmm = create_mess3_hmm()
            elif hmm.lower() == 'rrxor':
                self.hmm = create_rrxor_hmm()
            else:
                raise ValueError(f"Unknown HMM type: {hmm}")
        else:
            self.hmm = hmm

        self.num_sequences = num_sequences
        self.seq_length = seq_length
        self.include_beliefs = include_beliefs
        self.include_states = include_states

        # Create token mapping if not provided
        if token_to_idx is None:
            self.token_to_idx = {token: i for i, token in enumerate(self.hmm.tokens)}
        else:
            self.token_to_idx = token_to_idx

        self.idx_to_token = {i: token for token, i in self.token_to_idx.items()}
        self.vocab_size = len(self.token_to_idx)

        # Generate all sequences at initialization
        self.generated_data = self._generate_all_sequences()

    def _generate_all_sequences(self):
        """Generate all sequences and corresponding data"""
        all_data = []

        for _ in range(self.num_sequences):
            # Generate a sequence from the HMM
            tokens, states = self.hmm.generate_sequence(self.seq_length)

            # Convert tokens to indices
            token_indices = [self.token_to_idx[token] for token in tokens]

            # Create data dictionary for this sequence
            sequence_data = {
                'tokens': torch.tensor(token_indices, dtype=torch.long),
            }

            if self.include_states:
                sequence_data['states'] = torch.tensor(states, dtype=torch.long)

            if self.include_beliefs:
                # Compute belief sequence
                beliefs = self.hmm.compute_belief_sequence(tokens)
                sequence_data['beliefs'] = torch.tensor(beliefs, dtype=torch.float32)

            all_data.append(sequence_data)

        return all_data

    def __len__(self):
        return self.num_sequences

    def __getitem__(self, idx):
        return self.generated_data[idx]


class ParameterizableHMM(HMM):
    """
    A parameterizable HMM where we can specify:
    1. Number of states
    2. Size of vocabulary
    3. Sparsity of transitions
    """

    def __init__(
        self,
        n_states: int = 5,
        vocab_size: int = 3,
        sparsity: float = 0.5,
        concentration: float = 1.0,
        random_seed: Optional[int] = None
    ):
        """
        Initialize a parameterizable HMM.

        Args:
            n_states: Number of hidden states
            vocab_size: Size of token vocabulary
            sparsity: Probability of a transition being zero (0-1)
            concentration: Dirichlet concentration parameter (lower = more peaked)
            random_seed: Random seed for reproducibility
        """
        if random_seed is not None:
            np.random.seed(random_seed)

        # Create vocab
        tokens = [str(i) for i in range(vocab_size)]

        # Generate transition matrices
        transition_matrices = {}

        for token in tokens:
            # Create a sparse mask
            mask = np.random.random((n_states, n_states)) > sparsity

            # Generate Dirichlet samples for each row
            matrix = np.zeros((n_states, n_states))
            for i in range(n_states):
                # Only sample non-zero transitions
                nonzero_cols = np.where(mask[i])[0]
                if len(nonzero_cols) > 0:
                    row_values = np.random.dirichlet(
                        [concentration] * len(nonzero_cols)
                    )
                    matrix[i, nonzero_cols] = row_values

            transition_matrices[token] = matrix

        # Initialize the base HMM
        super().__init__(transition_matrices)


def create_hmm_dataloaders(
    hmm_type: str = 'mess3',
    custom_hmm_params: Optional[Dict] = None,
    train_size: int = 800,
    val_size: int = 100,
    test_size: int = 100,
    seq_length: int = 50,
    batch_size: int = 32,
    include_beliefs: bool = True,
    include_states: bool = True,
    seed: int = 42
):
    """
    Create train/val/test dataloaders for an HMM.

    Args:
        hmm_type: Type of HMM to use ('z1r', 'mess3', 'rrxor', 'custom')
        custom_hmm_params: Parameters for custom HMM (if hmm_type='custom')
        train_size: Number of training sequences
        val_size: Number of validation sequences
        test_size: Number of test sequences
        seq_length: Length of each sequence
        batch_size: Batch size for dataloaders
        include_beliefs: Whether to include belief states
        include_states: Whether to include true hidden states
        seed: Random seed

    Returns:
        Tuple of (train_loader, val_loader, test_loader, hmm)
    """
    # Set seeds
    np.random.seed(seed)
    torch.manual_seed(seed)

    # Create or select HMM
    if hmm_type.lower() == 'custom':
        if custom_hmm_params is None:
            custom_hmm_params = {}
        hmm = ParameterizableHMM(**custom_hmm_params, random_seed=seed)
    else:
        hmm = hmm_type  # Will be resolved in the HMMDataset constructor

    # Create datasets with different seeds
    train_dataset = HMMDataset(
        hmm=hmm,
        num_sequences=train_size,
        seq_length=seq_length,
        include_beliefs=include_beliefs,
        include_states=include_states,
        seed=seed
    )

    val_dataset = HMMDataset(
        hmm=train_dataset.hmm,  # Use the same HMM
        num_sequences=val_size,
        seq_length=seq_length,
        include_beliefs=include_beliefs,
        include_states=include_states,
        token_to_idx=train_dataset.token_to_idx,  # Use the same mapping
        seed=seed+1
    )

    test_dataset = HMMDataset(
        hmm=train_dataset.hmm,
        num_sequences=test_size,
        seq_length=seq_length,
        include_beliefs=include_beliefs,
        include_states=include_states,
        token_to_idx=train_dataset.token_to_idx,
        seed=seed+2
    )

    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False
    )

    return train_loader, val_loader, test_loader, train_dataset.hmm


# Example usage
if __name__ == "__main__":
    # Example 1: Using a predefined HMM
    train_loader, val_loader, test_loader, hmm = create_hmm_dataloaders(
        hmm_type='mess3',
        seq_length=30,
        batch_size=16
    )

    # Print information about the dataset
    print(f"Vocabulary size: {len(train_loader.dataset.token_to_idx)}")
    print(f"Number of states: {hmm.n_states}")
    print(f"Number of training batches: {len(train_loader)}")

    # Get a sample batch
    batch = next(iter(train_loader))
    print("\nSample batch:")
    for key, value in batch.items():
        print(f"{key} shape: {value.shape}")

    # Example 2: Using a custom parameterizable HMM
    custom_params = {
        'n_states': 8,
        'vocab_size': 5,
        'sparsity': 0.7,
        'concentration': 0.5
    }

    custom_train_loader, custom_val_loader, custom_test_loader, custom_hmm = create_hmm_dataloaders(
        hmm_type='custom',
        custom_hmm_params=custom_params,
        seq_length=50,
        batch_size=32
    )

    print("\nCustom HMM:")
    print(f"Vocabulary size: {len(custom_train_loader.dataset.token_to_idx)}")
    print(f"Number of states: {custom_hmm.n_states}")

    # Get a sample batch
    custom_batch = next(iter(custom_train_loader))
    print("\nSample batch from custom HMM:")
    for key, value in custom_batch.items():
        print(f"{key} shape: {value.shape}")

    # Example 3: Demonstrating token decoding
    token_indices = custom_batch['tokens'][0].numpy()
    tokens = [custom_train_loader.dataset.idx_to_token[idx] for idx in token_indices]
    print("\nFirst sequence from custom HMM:")
    print("".join(tokens))

    if 'beliefs' in custom_batch:
        beliefs = custom_batch['beliefs'][0]
        print(f"First belief state shape: {beliefs[0].shape}")
        print(f"First belief state: {beliefs[0].numpy()}")
